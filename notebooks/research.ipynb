{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total domains to be fetched: 21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching domains: 100%|██████████| 21/21 [00:21<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total fetched domains: 21.\n",
      "Total pages of all domains: 70972.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from meilisearch import Client\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "import json\n",
    "import concurrent.futures\n",
    "from urllib.error import HTTPError\n",
    "import os\n",
    "import pdfplumber\n",
    "\n",
    "# List of domain names\n",
    "domain_names = [\n",
    "    \"clubofbudapest.org\",\n",
    "    \"club-of-budapest.com\",\n",
    "    \"clubofbudapest.com\",\n",
    "    \"club-of-budapest.org\",\n",
    "    \"club-of-budapest.it\",\n",
    "    \"club-of-budapest.de\",\n",
    "    \"clubofbudapest.de\",\n",
    "    \"clubofbudapest.fw.hu\",\n",
    "    \"clubofbudapest.cz\",\n",
    "    \"cobusa.org\",\n",
    "    \"clubofbudapest.nl\",\n",
    "    \"clubofbudapest.ca\",\n",
    "    \"clubofbudapest.org.au\",\n",
    "    \"budapestklub.hu\",\n",
    "    \"clubofbudapest.hu\",\n",
    "    \"budapestklub.matav.hu\",\n",
    "    \"clubdebudapest.org\",\n",
    "    \"club-de-budapest.asso.fr\",\n",
    "    \"iwc.org.hu\",\n",
    "    \"worldshiftnetwork.org\",\n",
    "    \"globalspirit.org\"\n",
    "]\n",
    "\n",
    "opener = urllib.request.build_opener(\n",
    "    urllib.request.ProxyHandler(\n",
    "        {\n",
    "            'http': 'http://brd-customer-hl_6448c571-zone-zone1:d0rsfd3f67nx@brd.superproxy.io:22225',\n",
    "            'https': 'http://brd-customer-hl_6448c571-zone-zone1:d0rsfd3f67nx@brd.superproxy.io:22225'\n",
    "            }\n",
    "        )\n",
    ")\n",
    "\n",
    "def fetch_pages(domain):\n",
    "    url = f'http://web.archive.org/cdx/search/cdx?url={domain}&output=json&matchType=domain&fl=timestamp,original,mimetype,statuscode'\n",
    "    response = opener.open(url)\n",
    "    return response\n",
    "\n",
    "def process_domains(domain_names):\n",
    "    all_pages = []\n",
    "    domains_dict = {}  \n",
    "\n",
    "    print(f\"Total domains to be fetched: {len(domain_names)}...\")\n",
    "\n",
    "    for domain in tqdm(domain_names, desc=\"Fetching domains\"):\n",
    "        response = fetch_pages(domain)\n",
    "        \n",
    "        if response.status == 200:\n",
    "            response_json = json.load(response)\n",
    "            domains_dict[domain] = response_json[1:] # skip first row\n",
    "\n",
    "    print(f\"Total fetched domains: {len(domains_dict)}.\")\n",
    "    print(f\"Total pages of all domains: {sum([len(rows) for rows in domains_dict.values()])}.\")\n",
    "\n",
    "    return domains_dict\n",
    "\n",
    "def step_1():\n",
    "    domains_dict = process_domains(domain_names)\n",
    "\n",
    "    if not os.path.exists('../data'):\n",
    "        os.makedirs('../data')\n",
    "    \n",
    "    all_domains_pages = None\n",
    "\n",
    "    for domain, domain_pages in domains_dict.items():\n",
    "        # skip first row\n",
    "\n",
    "        domain_pages_df = pd.DataFrame(domain_pages, columns=['timestamp', 'original', 'mimetype', 'statuscode'])\n",
    "        domain_pages_df['domain'] = domain  \n",
    "        domain_pages_df.to_csv(f'../data/{domain}.csv', index=False)\n",
    "\n",
    "    for domain, domain_pages in domains_dict.items():    \n",
    "        all_domains_pages = pd.concat([pd.read_csv(f'../data/{domain}.csv') for domain in domain_names])\n",
    "\n",
    "    all_domains_pages.to_csv('../data/all_domains_pages.csv', index=False)\n",
    "\n",
    "    return domains_dict\n",
    "   \n",
    "domains_dict = step_1() \n",
    "df = pd.read_csv('../data/all_domains_pages.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/all_domains_pages.csv').head()\n",
    "# put domains_dict in a dataframe\n",
    "# df = pd.DataFrame()\n",
    "# for domain in domains_dict:\n",
    "#     df = df.append(pd.DataFrame(domains_dict[domain], columns=['timestamp', 'original', 'mimetype', 'statuscode']))\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages for each domain: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages for each domain: 2it [00:21, 10.60s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>wayback_machine_url</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>original_content</th>\n",
       "      <th>mimetype</th>\n",
       "      <th>statuscode</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clubofbudapest.org</td>\n",
       "      <td>20041020184009</td>\n",
       "      <td>http://web.archive.org/web/20041020184009/http...</td>\n",
       "      <td>http://www.clubofbudapest.org:80/BestProjects/...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSUPPORT\\n\\n\\n\\n\\...</td>\n",
       "      <td>b'&lt;html&gt;\\n\\n&lt;head&gt;&lt;script src=\"//archive.org/i...</td>\n",
       "      <td>text/html</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clubofbudapest.org</td>\n",
       "      <td>20160323233557</td>\n",
       "      <td>http://web.archive.org/web/20160323233557/http...</td>\n",
       "      <td>http://clubofbudapest.org/clubofbudapest/attac...</td>\n",
       "      <td>THE FUJI DECLARATION\\nAwakening The Divine Spa...</td>\n",
       "      <td>b'%PDF-1.3\\n%\\xc4\\xe5\\xf2\\xe5\\xeb\\xa7\\xf3\\xa0\\...</td>\n",
       "      <td>application/pdf</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               domain       timestamp  \\\n",
       "0  clubofbudapest.org  20041020184009   \n",
       "1  clubofbudapest.org  20160323233557   \n",
       "\n",
       "                                 wayback_machine_url  \\\n",
       "0  http://web.archive.org/web/20041020184009/http...   \n",
       "1  http://web.archive.org/web/20160323233557/http...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://www.clubofbudapest.org:80/BestProjects/...   \n",
       "1  http://clubofbudapest.org/clubofbudapest/attac...   \n",
       "\n",
       "                                                text  \\\n",
       "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSUPPORT\\n\\n\\n\\n\\...   \n",
       "1  THE FUJI DECLARATION\\nAwakening The Divine Spa...   \n",
       "\n",
       "                                    original_content         mimetype  \\\n",
       "0  b'<html>\\n\\n<head><script src=\"//archive.org/i...        text/html   \n",
       "1  b'%PDF-1.3\\n%\\xc4\\xe5\\xf2\\xe5\\xeb\\xa7\\xf3\\xa0\\...  application/pdf   \n",
       "\n",
       "   statuscode  id  \n",
       "0         200   1  \n",
       "1         200   2  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = None\n",
    "download_folder = '../data/pdfs'\n",
    "all_domains_pages_df = None\n",
    "all_domains_pages_csv = '../data/test.csv'\n",
    "\n",
    "def read_pdf_content(pdf_path):\n",
    "    pdf_content = ''\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            pdf_content += page.extract_text() + ' '\n",
    "    return pdf_content\n",
    "\n",
    "def fetch_pdf_content(original_content, pdf_url):\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "\n",
    "    filename = os.path.join(download_folder, pdf_url.split('/')[-1])\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(original_content)\n",
    "\n",
    "    return read_pdf_content(filename)\n",
    "\n",
    "def fetch_html_content(original_content):\n",
    "    soup = BeautifulSoup(original_content, 'html.parser')\n",
    "    text = soup.get_text()    \n",
    "    return text \n",
    "\n",
    "def fetch_content(wayback_machine_url):\n",
    "    try:\n",
    "        response = opener.open(wayback_machine_url)\n",
    "        if response.status == 200:\n",
    "            return response.read()\n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTP Error: {e} for URL: {wayback_machine_url}\")\n",
    "        \n",
    "    return None\n",
    "   \n",
    "def process_domain(domain, rows):  \n",
    "    print(f\"{domain} ({len(rows)}):\")\n",
    "    domain_pages = fetch_domain_pages(domain, rows)    \n",
    "    print(f\"{domain}: {(len(rows) - len(domain_pages))} of {len(rows)} fetched.\")\n",
    "    save_domain_pages(domain, domain_pages)\n",
    "    return domain_pages\n",
    "\n",
    "def step_2():\n",
    "    pages = pd.read_csv(all_domains_pages_csv)\n",
    "\n",
    "    pages_content = pd.DataFrame(columns=['domain', 'timestamp', 'wayback_machine_url', 'url', 'text', 'original_content', 'mimetype', 'statuscode'])\n",
    "    pages_content_list = []\n",
    "    for index, row in tqdm(pages.iterrows(), desc=\"Fetching pages for each domain\"):\n",
    "        timestamp = row.iloc[0]\n",
    "        url = row.iloc[1]\n",
    "        wayback_machine_url = f'http://web.archive.org/web/{timestamp}/{url}'\n",
    "        mimetype = row.iloc[2]\n",
    "        statuscode = row.iloc[3]\n",
    "\n",
    "        if row['mimetype'] == 'application/pdf':\n",
    "            wayback_machine_pdf_url = f'http://web.archive.org/web/{timestamp}if_/{url}'\n",
    "            original_content = fetch_content(wayback_machine_pdf_url)\n",
    "            text = fetch_pdf_content(original_content, url)\n",
    "        else:\n",
    "            original_content = fetch_content(wayback_machine_url)\n",
    "            text = fetch_html_content(original_content)   \n",
    "\n",
    "        pages_content_list.append({\n",
    "            'domain': row['domain'],\n",
    "            'timestamp': timestamp,\n",
    "            'wayback_machine_url': wayback_machine_url,\n",
    "            'url': url,\n",
    "            'text': text,\n",
    "            'original_content': original_content,\n",
    "            'mimetype': mimetype,\n",
    "            'statuscode': statuscode\n",
    "        })\n",
    "\n",
    "    pages_content = pd.DataFrame(pages_content_list)\n",
    "    pages_content['id'] = range(1, len(pages_content) + 1)\n",
    "    return pages_content\n",
    "\n",
    "pages_content = step_2()  \n",
    "pages_content.to_pickle('../data/pages_content.pkl')\n",
    "pages_content.to_csv('../data/pages_content.csv', escapechar='\\\\')\n",
    "pages_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clubofbudapest.de\n",
      "http://www.clubofbudapest.de:80/\n",
      "http://web.archive.org/web/20040609191801/http://www.clubofbudapest.de:80/\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_3():\n",
    "    if not df:\n",
    "        if not os.path.exists('data/pages.json'):\n",
    "            print(\"data/pages.csv not found. Please run step 2 first.\")\n",
    "            return\n",
    "        df = pd.read_json('data/pages.json')\n",
    "\n",
    "    # Ingest the dataframe into a MeiliSearch index in batches of 100\n",
    "    client = Client('http://localhost:7700', 'masterKey')\n",
    "    index = client.index('pages')\n",
    "\n",
    "    batch_size = 100\n",
    "    documents = df.to_dict(orient='records')\n",
    "    total_documents = len(documents)\n",
    "\n",
    "    with tqdm(total=total_documents, desc=\"Adding pages to index\") as pbar:\n",
    "        for i in range(0, total_documents, batch_size):\n",
    "            batch = documents[i:i+batch_size]\n",
    "            index.add_documents(batch)\n",
    "            pbar.update(len(batch))\n",
    "\n",
    "    print(\"Indexing completed.\")\n",
    "\n",
    "step_3() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
