{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from meilisearch import Client\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "import json\n",
    "import concurrent.futures\n",
    "from urllib.error import HTTPError\n",
    "import os\n",
    "import pdfplumber\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from io import BytesIO\n",
    "\n",
    "class BaseContent(BaseModel):\n",
    "    text: str\n",
    "    title: Optional[str]\n",
    "\n",
    "class Page(BaseModel):\n",
    "    timestamp: str\n",
    "    original: str\n",
    "    mimetype: str\n",
    "    statuscode: str\n",
    "    digest: str\n",
    "    length: str\n",
    "\n",
    "class DomainPages(BaseModel):\n",
    "    domain: str\n",
    "    pages: List[Page]\n",
    "\n",
    "class PageContent(BaseModel):\n",
    "    id: Optional[int]\n",
    "    title: str\n",
    "    domain: str\n",
    "    timestamp: str\n",
    "    unix_timestamp: Optional[int]\n",
    "    wayback_machine_url: str\n",
    "    url: str\n",
    "    text: str\n",
    "    mimetype: str\n",
    "\n",
    "class DomainPagesContent(BaseModel):\n",
    "    domain: str\n",
    "    pages_contents: List[PageContent]\n",
    "\n",
    "# List of domain names\n",
    "domain_names = [\n",
    "    \"clubofbudapest.org.au\",\n",
    "    \"clubofbudapest.fw.hu\",\n",
    "    \"clubofbudapest.de\",\n",
    "    \"iwc.org.hu\",\n",
    "    \"clubofbudapest.hu\",\n",
    "    \"budapestklub.matav.hu\",\n",
    "    \"clubofbudapest.cz\",\n",
    "    \"clubofbudapest.ca\",\n",
    "    \"club-of-budapest.de\",\n",
    "    \"cobusa.org\",\n",
    "    \"clubdebudapest.org\",\n",
    "    \"worldshiftnetwork.org\",\n",
    "    \"globalspirit.org\",\n",
    "    \"club-of-budapest.org\",\n",
    "    \"budapestklub.hu\",\n",
    "    \"club-de-budapest.asso.fr\",\n",
    "    \"club-of-budapest.com\",\n",
    "    \"club-of-budapest.it\",\n",
    "    \"clubofbudapest.com\",\n",
    "    \"clubofbudapest.org\"\n",
    "]\n",
    "# domain_names = [\n",
    "#     \"clubofbudapest.fw.hu\",\n",
    "#     \"clubofbudapest.de\",\n",
    "# ]\n",
    "\n",
    "opener = urllib.request.build_opener(\n",
    "    urllib.request.ProxyHandler(\n",
    "        {\n",
    "            'http': 'http://brd-customer-hl_6448c571-zone-zone1:d0rsfd3f67nx@brd.superproxy.io:22225',\n",
    "            'https': 'http://brd-customer-hl_6448c571-zone-zone1:d0rsfd3f67nx@brd.superproxy.io:22225'\n",
    "            }\n",
    "        )\n",
    ")\n",
    "\n",
    "df = None\n",
    "data_folder = '../data'\n",
    "download_folder = data_folder + '/pdfs'\n",
    "all_domains_pages_df = None\n",
    "meilisearch_host = 'http://127.0.0.1:7700'  # Replace with your Meilisearch host\n",
    "index_name = 'club-of-budapest-6' \n",
    "meilisearch_api_key = 'masterKey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching domains:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching domains: 100%|██████████| 20/20 [00:02<00:00,  7.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "import json\n",
    "\n",
    "def fetch_pages(domain: str) -> Tuple[str, Optional[bytes]]:\n",
    "    url = f'https://web.archive.org/cdx/search/cdx?url={domain}&output=json&collapse=digest&matchType=domain&fl=timestamp,original,mimetype,statuscode,digest,length&filter=statuscode:200&filter=mimetype:text/html|application/pdf'\n",
    "    response = opener.open(url)\n",
    "    return domain, response\n",
    "\n",
    "def convert_json_to_page(json_data: List[List[str]]) -> List[Page]:\n",
    "    pages = []\n",
    "    for page in json_data[1:]:\n",
    "        page = Page(\n",
    "            timestamp=page[0],\n",
    "            original=page[1],\n",
    "            mimetype=page[2],\n",
    "            statuscode=page[3],\n",
    "            digest=page[4],\n",
    "            length=page[5]\n",
    "        )\n",
    "        pages.append(page)\n",
    "    return pages\n",
    "\n",
    "def fetch_all_domain_pages() -> List[DomainPages]:\n",
    "    all_domain_pages = [] \n",
    "\n",
    "    print(f\"Total domains to be fetched: {len(domain_names)}...\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "\n",
    "        for domain in domain_names:\n",
    "            future = executor.submit(fetch_pages, domain)\n",
    "            futures.append(future)\n",
    "            \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), desc=\"Fetching domains\", total=len(domain_names)):\n",
    "            domain, response = future.result()\n",
    "            \n",
    "            if response is None:\n",
    "                continue\n",
    "\n",
    "            response_json = json.load(response)\n",
    "            pages = convert_json_to_page(response_json)\n",
    "            domain_pages = DomainPages(domain=domain, pages=pages)\n",
    "\n",
    "            all_domain_pages.append(domain_pages)\n",
    "\n",
    "    # reorder all_domain_pages by pages length from smallest to largest\n",
    "    all_domain_pages = sorted(all_domain_pages, key=lambda x: len(x.pages))\n",
    "\n",
    "    print(f\"Total fetched domains: {len(all_domain_pages)}.\")\n",
    "    print(f\"Total pages fetched: {sum([len(domain_pages.pages) for domain_pages in all_domain_pages])}.\")\n",
    "\n",
    "    return all_domain_pages\n",
    "\n",
    "all_domain_pages = fetch_all_domain_pages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_pdf_content(pdf_path: str) -> Optional[BaseContent]:\n",
    "    pdf_content = ''\n",
    "    pdf_title = ''\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            pdf_title = pdf.metadata['title']\n",
    "            for page in pdf.pages:\n",
    "                pdf_content += page.extract_text() + ' '\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    return BaseContent(text=pdf_content, title=pdf_title)\n",
    "\n",
    "def fetch_pdf_content(original_content: bytes, pdf_url: str) -> Optional[BaseContent]:\n",
    "    download_folder = 'downloads'  # Assuming the download folder exists\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "\n",
    "    filename = os.path.join(download_folder, pdf_url.split('/')[-1])\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(original_content)\n",
    "\n",
    "    return read_pdf_content(filename)\n",
    "\n",
    "\n",
    "def fetch_pdf_content_on_the_fly(content, url):\n",
    "    pdf_content = ''\n",
    "    pdf_title = ''\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(BytesIO(content)) as pdf:\n",
    "            pdf_content = ' '.join(page.extract_text() for page in pdf.pages)\n",
    "            if pdf.metadata is not None and 'title' in pdf.metadata:\n",
    "                pdf_title = pdf.metadata['title']\n",
    "\n",
    "        if pdf_title == '':\n",
    "            pdf_title = url.split('/')[-1]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    return BaseContent(text=pdf_content, title=pdf_title)\n",
    "\n",
    "def fetch_html_content(original_content: bytes) -> Optional[BaseContent]:\n",
    "    soup = BeautifulSoup(original_content, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    text = text.replace('\\n', ' ').replace(' +', ' ')\n",
    "    title = soup.title.string if soup.title else ''\n",
    "\n",
    "    return BaseContent(text=text, title=title)\n",
    "\n",
    "def fetch_content(wayback_machine_url: str) -> Optional[bytes]:\n",
    "    try:\n",
    "        response = opener.open(wayback_machine_url)\n",
    "        if response.status == 200:\n",
    "            return response.read()\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    "   \n",
    "def process_page(page: Page, domain: str) -> Optional[PageContent]:\n",
    "    wayback_machine_url = f'https://web.archive.org/web/{page.timestamp}/{page.original}'\n",
    "    wayback_machine_content_url = f'https://web.archive.org/web/{page.timestamp}if_/{page.original}'\n",
    "\n",
    "    if page.mimetype == 'application/pdf':\n",
    "        original_content = fetch_content(wayback_machine_content_url)   \n",
    "        \n",
    "        if original_content is None:\n",
    "            return None\n",
    "        content = fetch_pdf_content_on_the_fly(original_content, page.original)\n",
    "    elif page.mimetype == 'text/html':\n",
    "        original_content = fetch_content(wayback_machine_content_url)     \n",
    "        if original_content is None:       \n",
    "            return None\n",
    "\n",
    "        content = fetch_html_content(original_content)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    if content is None:\n",
    "        return None\n",
    "        \n",
    "    if content.text is None or len(content.text) < 500:\n",
    "        return None\n",
    "\n",
    "    unix_timestamp = int(pd.to_datetime(page.timestamp).timestamp())\n",
    "\n",
    "    return PageContent(\n",
    "        title=content.title,\n",
    "        domain=domain,\n",
    "        timestamp=page.timestamp,\n",
    "        unix_timestamp=unix_timestamp,\n",
    "        wayback_machine_url=wayback_machine_url,\n",
    "        url=page.original,\n",
    "        text=content.text,\n",
    "        mimetype=page.mimetype\n",
    "    )\n",
    "\n",
    "def fetch_domain_pages_content(all_domain_pages: List[DomainPages], use_threads: bool = False) -> List[DomainPagesContent]:\n",
    "    domain_pages_content_list = []\n",
    "\n",
    "    if use_threads:\n",
    "        for domain_pages in all_domain_pages:\n",
    "            domain_pages_content = DomainPagesContent(domain=domain_pages.domain, pages_contents=[])\n",
    "\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                futures = []\n",
    "\n",
    "                for page in domain_pages.pages:\n",
    "                    future = executor.submit(process_page, page, domain_pages.domain)\n",
    "                    futures.append(future)\n",
    "\n",
    "                for future in tqdm(concurrent.futures.as_completed(futures), desc=f\"Fetching pages for domain '{domain_pages.domain}'\", total=len(domain_pages.pages)):\n",
    "                    result = future.result()\n",
    "                    if result is not None:\n",
    "                        domain_pages_content.pages_contents.append(result)\n",
    "\n",
    "                domain_pages_content_list.append(domain_pages_content)\n",
    "    else:\n",
    "        for domain_pages in all_domain_pages:\n",
    "            domain_pages_content = DomainPagesContent(domain=domain_pages.domain, pages_contents=[])\n",
    "\n",
    "            for page in tqdm(domain_pages.pages, desc=f\"Fetching pages for domain '{domain_pages.domain}'\", total=len(domain_pages.pages)):\n",
    "                result = process_page(page, domain_pages.domain)\n",
    "                if result is not None:\n",
    "                    domain_pages_content.pages_contents.append(result)\n",
    "\n",
    "            domain_pages_content_list.append(domain_pages_content)\n",
    "\n",
    "    return domain_pages_content_list\n",
    "\n",
    "def create_index(all_domain_pages_content: List[DomainPagesContent], index_name: str) -> None:    \n",
    "    # init meilisearch\n",
    "    client = Client(meilisearch_host, meilisearch_api_key)\n",
    "    client.create_index(index_name, {'primaryKey': 'id'})\n",
    "\n",
    "    # iterate over pages_content and add to Meilisearch index\n",
    "    all_pages_content = []\n",
    "    # count total pages in pages_content\n",
    "\n",
    "    index = client.index(index_name)\n",
    "    index.update_filterable_attributes([\n",
    "        'unix_timestamp',\n",
    "        'domain',\n",
    "    ])\n",
    "    total_pages = 0\n",
    "\n",
    "    for domain_pages_content in all_domain_pages_content:\n",
    "        print(f\"Adding {len(domain_pages_content.pages_contents)} pages from domain '{domain_pages_content.domain}' to Meilisearch index...\")\n",
    "        total_pages += len(domain_pages_content.pages_contents)\n",
    "        all_pages_content.extend(domain_pages_content.pages_contents)\n",
    "\n",
    "    # update all_pages_content and set id as index starting with 1\n",
    "    for i, page_content in enumerate(all_pages_content):\n",
    "        page_content.id = i + 1\n",
    "                \n",
    "    # convert all_pages_content to dictoionary\n",
    "    all_pages_content_dict = [page_content.dict() for page_content in all_pages_content]\n",
    "\n",
    "    # add all_pages_content_dict to Meilisearch index\n",
    "    index.add_documents(all_pages_content_dict)\n",
    "\n",
    "    print(f\"Total pages added to Meilisearch index: {total_pages}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages for domain 'clubofbudapest.org.au':   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages for domain 'clubofbudapest.org.au': 100%|██████████| 16/16 [00:02<00:00,  6.49it/s]\n",
      "Fetching pages for domain 'clubofbudapest.de': 100%|██████████| 18/18 [00:02<00:00,  7.21it/s]\n",
      "Fetching pages for domain 'clubofbudapest.fw.hu': 100%|██████████| 31/31 [00:05<00:00,  6.02it/s]\n",
      "Fetching pages for domain 'clubofbudapest.hu': 100%|██████████| 164/164 [00:13<00:00, 12.45it/s]\n",
      "Fetching pages for domain 'clubofbudapest.ca': 100%|██████████| 182/182 [00:18<00:00,  9.91it/s]\n",
      "Fetching pages for domain 'budapestklub.matav.hu': 100%|██████████| 209/209 [00:13<00:00, 15.20it/s]\n",
      "Fetching pages for domain 'iwc.org.hu': 100%|██████████| 270/270 [00:45<00:00,  5.96it/s]\n",
      "Fetching pages for domain 'cobusa.org': 100%|██████████| 313/313 [00:30<00:00, 10.24it/s]\n",
      "Fetching pages for domain 'club-de-budapest.asso.fr': 100%|██████████| 335/335 [00:22<00:00, 14.95it/s]\n",
      "Fetching pages for domain 'club-of-budapest.de': 100%|██████████| 371/371 [00:25<00:00, 14.28it/s]\n",
      "Fetching pages for domain 'globalspirit.org':  36%|███▌      | 133/374 [00:10<00:16, 14.45it/s]/workspaces/wayback-machine-keyword-search/.venv/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "Fetching pages for domain 'globalspirit.org':  85%|████████▌ | 318/374 [00:23<00:05,  9.76it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org':  86%|████████▌ | 322/374 [00:23<00:04, 10.68it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org':  87%|████████▋ | 324/374 [00:23<00:04, 11.06it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org':  87%|████████▋ | 327/374 [00:23<00:03, 13.86it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org':  88%|████████▊ | 329/374 [00:23<00:03, 14.82it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org':  89%|████████▊ | 331/374 [00:24<00:04, 10.38it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org':  89%|████████▉ | 333/374 [00:24<00:04,  9.94it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org':  90%|████████▉ | 335/374 [00:24<00:03, 10.43it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org':  90%|█████████ | 337/374 [00:24<00:03, 12.06it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org':  91%|█████████ | 340/374 [00:24<00:02, 15.56it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org':  91%|█████████▏| 342/374 [00:24<00:02, 13.76it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org':  92%|█████████▏| 344/374 [00:25<00:02, 14.80it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org':  93%|█████████▎| 348/374 [00:25<00:01, 19.34it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org':  94%|█████████▍| 351/374 [00:25<00:01, 19.79it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org':  95%|█████████▍| 354/374 [00:25<00:01, 18.94it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org':  99%|█████████▉| 371/374 [00:26<00:00, 11.51it/s]Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Fetching pages for domain 'globalspirit.org': 100%|██████████| 374/374 [00:36<00:00, 10.24it/s]\n",
      "Fetching pages for domain 'clubofbudapest.cz': 100%|██████████| 613/613 [01:39<00:00,  6.16it/s]\n",
      "Fetching pages for domain 'club-of-budapest.org': 100%|██████████| 943/943 [01:12<00:00, 13.00it/s]\n",
      "Fetching pages for domain 'clubdebudapest.org': 100%|██████████| 1017/1017 [01:27<00:00, 11.62it/s]\n",
      "Fetching pages for domain 'budapestklub.hu': 100%|██████████| 1191/1191 [01:43<00:00, 11.54it/s]\n",
      "Fetching pages for domain 'clubofbudapest.com': 100%|██████████| 1462/1462 [04:10<00:00,  5.84it/s]\n",
      "Fetching pages for domain 'worldshiftnetwork.org': 100%|██████████| 1505/1505 [02:42<00:00,  9.26it/s]\n",
      "Fetching pages for domain 'club-of-budapest.com': 100%|██████████| 1945/1945 [02:11<00:00, 14.78it/s]\n",
      "Fetching pages for domain 'club-of-budapest.it': 100%|██████████| 2075/2075 [04:30<00:00,  7.67it/s]\n",
      "Fetching pages for domain 'clubofbudapest.org': 100%|██████████| 3133/3133 [04:18<00:00, 12.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# domain_pages_content_list = fetch_domain_pages_content(all_domain_pages)\n",
    "# print(\"domain_pages_content_list\", len(domain_pages_content_list))\n",
    "\n",
    "threaded_domain_pages_content_list = fetch_domain_pages_content(all_domain_pages, use_threads=True)\n",
    "print(\"threaded_domain_pages_content_list\", len(threaded_domain_pages_content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index(threaded_domain_pages_content_list, index_name=\"club-of-budapest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
